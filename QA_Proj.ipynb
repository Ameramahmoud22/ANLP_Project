{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7cee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<!-- ########################################################## -->\n",
      "<!-- #                                                          -->\n",
      "<!-- #  arwiki-20180920.xml                                     -->\n",
      "<!-- #                                                          -->\n",
      "<!-- #                                                          -->\n",
      "<!-- #  (C) Copyright 2018 Wikimedia Foundation, Inc.           -->\n",
      "<!-- #                        http://ar.wikipedia.org           -->\n",
      "<!-- #  (C) Copyright this adaption: Peter Kolb                 -->\n",
      "<!-- #               peter.kolb@linguatools.org                 -->\n",
      "<!-- #               http://www.linguatools.org/tools/corpora/  -->\n",
      "<!-- #                                                          -->\n",
      "<!-- #  This work is made available under the Creative Commons  -->\n",
      "<!-- #  Attribution-ShareAlike 3.0 License:                     -->\n",
      "<!-- #  http://creativecommons.org/licenses/by-sa/3.0/legalcode -->\n",
      "<!-- #   \n"
     ]
    }
   ],
   "source": [
    "file_path = \"arwiki-20180920-corpus.xml\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:1000])  # Print first 1000 characters to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b8ba86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "# Download NLTK Arabic resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize Arabic stopwords and stemmer\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "def preprocess_arabic(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove non-Arabic characters (keep only Arabic letters and spaces)\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize (split by spaces)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in arabic_stopwords]\n",
    "    \n",
    "    # Apply stemming\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5078a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_paragraphs = []\n",
    "\n",
    "file_path = 'arwiki-20180920-corpus.xml'  \n",
    "\n",
    "# Define limit\n",
    "MAX_LINES = 10000  # Change this number if you want to read more or fewer lines\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i >= MAX_LINES:  # Stop after reading MAX_LINES\n",
    "            break\n",
    "        line = line.strip()\n",
    "        if line:  # Skip empty lines\n",
    "            clean_line = preprocess_arabic(line)\n",
    "            clean_paragraphs.append(clean_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "243a8a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Finished preprocessing! Total cleaned paragraphs: 9987\n"
     ]
    }
   ],
   "source": [
    "print(f\" Finished preprocessing! Total cleaned paragraphs: {len(clean_paragraphs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd90230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9d33b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Handle Missing Data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Check for missing values\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Fill missing values with empty string (for text)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Handle Missing Data\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing values with empty string (for text)\n",
    "df = df.fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb518bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "461f38ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c0b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags (if any)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a36c1daa",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3335994467.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [13]\u001b[1;36m\u001b[0m\n\u001b[1;33m    tokens = word_tokenize(text)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54c61f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           title                                            context  \\\n",
      "0  Super_Bowl_50  Super Bowl 50 was an American football game to...   \n",
      "1  Super_Bowl_50  Super Bowl 50 was an American football game to...   \n",
      "2  Super_Bowl_50  Super Bowl 50 was an American football game to...   \n",
      "3  Super_Bowl_50  Super Bowl 50 was an American football game to...   \n",
      "4  Super_Bowl_50  Super Bowl 50 was an American football game to...   \n",
      "\n",
      "                                            question                   answer  \\\n",
      "0  Which NFL team represented the AFC at Super Bo...           Denver Broncos   \n",
      "1  Which NFL team represented the NFC at Super Bo...        Carolina Panthers   \n",
      "2                 Where did Super Bowl 50 take place  Santa Clara, California   \n",
      "3                   Which NFL team won Super Bowl 50           Denver Broncos   \n",
      "4  What color was used to emphasize the 50th anni...                     gold   \n",
      "\n",
      "                         id        title                   answer   \n",
      "0  56be4db0acb8001400a502ec  Super_Bowl_50          Denver Broncos  \n",
      "1  56be4db0acb8001400a502ed  Super_Bowl_50       Carolina Panthers  \n",
      "2  56be4db0acb8001400a502ee  Super_Bowl_50  Santa Clara California  \n",
      "3  56be4db0acb8001400a502ef  Super_Bowl_50          Denver Broncos  \n",
      "4  56be4db0acb8001400a502f0  Super_Bowl_50                    gold  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee165ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e4fae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17385f8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[0;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle  \u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[0;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['question'] = df['question'].apply(clean_text)\n",
    "df['title  '] = df['title'].apply(clean_text)\n",
    "df['context'] = df['context'].apply(clean_text)\n",
    "df['answer '] = df['answer'].apply(clean_text)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
