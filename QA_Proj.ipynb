{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7cee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<!-- ########################################################## -->\n",
      "<!-- #                                                          -->\n",
      "<!-- #  arwiki-20180920.xml                                     -->\n",
      "<!-- #                                                          -->\n",
      "<!-- #                                                          -->\n",
      "<!-- #  (C) Copyright 2018 Wikimedia Foundation, Inc.           -->\n",
      "<!-- #                        http://ar.wikipedia.org           -->\n",
      "<!-- #  (C) Copyright this adaption: Peter Kolb                 -->\n",
      "<!-- #               peter.kolb@linguatools.org                 -->\n",
      "<!-- #               http://www.linguatools.org/tools/corpora/  -->\n",
      "<!-- #                                                          -->\n",
      "<!-- #  This work is made available under the Creative Commons  -->\n",
      "<!-- #  Attribution-ShareAlike 3.0 License:                     -->\n",
      "<!-- #  http://creativecommons.org/licenses/by-sa/3.0/legalcode -->\n",
      "<!-- #   \n"
     ]
    }
   ],
   "source": [
    "file_path = \"arwiki-20180920-corpus.xml\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:1000])  # Print first 1000 characters to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8ba86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re                    #تستخدم للبحث عن أنماط نصية وتعديلها\n",
    "import nltk        #توفر أدوات لمعالجة النصوص وتحليلها\n",
    "from nltk.corpus import stopwords           #تحتوي على كلمات شائعة في اللغة (مثل \"و\"، \"في\"، \"إلى\") التي عادةً ما تُزال أثناء معالجة النصوص\n",
    "from nltk.stem.isri import ISRIStemmer    #تحويل الكلمات إلى جذورها الأساسية (مثال: \"يكتبون\" → \"كتب\")\n",
    "\n",
    "# Download NLTK Arabic resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize Arabic stopwords and stemmer\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "def preprocess_arabic(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)   #يبحث عن أي نص بين علامتي <> ويحذفه>\n",
    "    \n",
    "    # Remove non-Arabic characters (keep only Arabic letters and spaces)\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)      # \\s يعني المسافات والأسطر الجديدة (يُبقيها)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()         #حذف المسافات من بداية ونهاية النص\n",
    "    \n",
    "    # Tokenize (split by spaces)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in arabic_stopwords]\n",
    "    \n",
    "    # Apply stemming  [ISRIStemmer] مخصص للعربية\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)    #جمع الكلمات مرة أخرى في نص واحد مفصول بمسافات\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_paragraphs = []\n",
    "\n",
    "file_path = 'arwiki-20180920-corpus.xml' \n",
    "\n",
    "MAX_LINES = 10000  # Limit lines for quick testing\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:   # r -> read only file  ,encoding='utf-8' يضمن قراءة النص العربي بشكل صحيح (بدون تشوهات)\n",
    "    for i, line in enumerate(file):  #تم معالجة كل سطر على حدة\n",
    "        if i >= MAX_LINES:  #التوقف عند الوصول إلى الحد الأقصى\n",
    "            break\n",
    "        line = line.strip()    #strip() يحذف المسافات والأسطر الجديدة من بداية ونهاية السطر\n",
    "\n",
    "        # Skip XML tags, HTML comments, and empty lines\n",
    "        if line.startswith('<?xml') or line.startswith('<!--') or line.startswith('<') or line == '':\n",
    "            continue\n",
    "\n",
    "        # Now clean and save real Arabic text\n",
    "        clean_line = preprocess_arabic(line)   #يستدعي الدالة preprocess_arabic() (التي شرحناها سابقاً) لتنظيف السطر الحالي.\n",
    "        \n",
    "        # Only add non-empty cleaned text\n",
    "        if clean_line.strip() != '':                     #يتأكد أن السطر النظيف ليس فارغاً بعد التنظيف\n",
    "            clean_paragraphs.append(clean_line)          #ذا كان النص غير فارغ، يُضاف إلى القائمة "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "243a8a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Finished preprocessing! Total cleaned paragraphs: 814\n"
     ]
    }
   ],
   "source": [
    "print(f\" Finished preprocessing! Total cleaned paragraphs: {len(clean_paragraphs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2637172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame  جدول بيانات\n",
    "df = pd.DataFrame({'paragraph': clean_paragraphs})  # يحتوي على كل الفقراتparagraphيتم إنشاء عمود واحد باسم\n",
    "df = df.fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7f56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned Arabic sample saved successfully to cleaned_arabic_wiki_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned sample\n",
    "df.to_csv('cleaned_arabic_wiki_sample.csv', index=False, encoding='utf-8')  #index=False: لا يحفظ أرقام الصفوف (التصنيف التلقائي)\n",
    "\n",
    "print(\" Cleaned Arabic sample saved successfully to cleaned_arabic_wiki_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6144bf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample Context:\n",
      " زخر ياه ارض سطح بعد شكل تنع حيي يه، نقص زدد عمق عمد حية ائة بحر حيط وجد ماء كعمل سسي كتل حيوية، ويك عمل حدد ناج كمة قدر غذي نبت ذاب وسف ركب ترج مونيوم نتر ضفة ثني كسد كرب\n"
     ]
    }
   ],
   "source": [
    "# Print a sample paragraph\n",
    "if len(clean_paragraphs) > 0:\n",
    "    print(\"\\n Sample Context:\\n\", clean_paragraphs[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2288ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['paragraph'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715078a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(814, 4111)\n",
      "['آبر' 'آتم' 'آخر' ... 'يوم' 'يون' 'يوه']\n",
      "Cleaned Arabic sample saved successfully to cleaned_arabic_wiki_sample.csv\n",
      "\n",
      "Sample Context:\n",
      " زخر ياه ارض سطح بعد شكل تنع حيي يه، نقص زدد عمق عمد حية ائة بحر حيط وجد ماء كعمل سسي كتل حيوية، ويك عمل حدد ناج كمة قدر غذي نبت ذاب وسف ركب ترج مونيوم نتر ضفة ثني كسد كرب\n"
     ]
    }
   ],
   "source": [
    "#السبب: لتحويل النصوص إلى مصفوفة رقمية باستخدام خوارزمية TF-IDF -> يعطي وزنًا أكبر للكلمات المهمة والنادرة , يقلل من تأثير الكلمات الشائعة (مثل \"في\"، \"من\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = df['paragraph'].tolist()  #   DataFrameإلى قائمة نصوص'paragraphالوظيفة: يحول عمود  في  \n",
    "\n",
    "# Initialize TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus)  #  TF (تكرار الكلمة): كم مرة تظهر الكلمة في الوثيقة الحالية , IDF (الأهمية): كم هي نادرة الكلمة عبر جميع الوثائق\n",
    "\n",
    "print(X.shape)  # يطبع (عدد الوثائق، عدد الكلمات الفريدة)  , ثال: (100, 5000) يعني 100 وثيقة و5000 كلمة مختلفة\n",
    "print(tfidf.get_feature_names_out())  # See the vocabulary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05310d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db32f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02465982  0.02210858  0.02295838  0.00122577  0.00963704 -0.07336017\n",
      "  0.04456649  0.10314479 -0.03340747 -0.02782421 -0.03188172 -0.05230464\n",
      " -0.02025422  0.01345363 -0.00024046 -0.04092807 -0.00242839 -0.04591601\n",
      "  0.00911411 -0.09710969  0.01579804  0.01291465  0.03608207 -0.01952653\n",
      " -0.00371001 -0.0189515  -0.04029905 -0.04442123 -0.03824596 -0.00056775\n",
      "  0.04286265  0.01902711  0.00808944 -0.03060136 -0.00857342  0.0588086\n",
      " -0.01053068 -0.03707424 -0.02983358 -0.08111031 -0.01265097 -0.04168354\n",
      " -0.0084947   0.01928122  0.04559947 -0.03364554 -0.03676334 -0.00363762\n",
      "  0.04084597  0.04047025  0.02170288 -0.0373952   0.00470576  0.01812375\n",
      " -0.04285415  0.04288952  0.00936949  0.00376055 -0.05506981  0.01030207\n",
      "  0.01562598  0.02727741 -0.01852809 -0.02495131 -0.07165431  0.02490312\n",
      "  0.02929877  0.04068363 -0.05740259  0.04528158 -0.0264707   0.02896178\n",
      "  0.04488745 -0.02868368  0.04104752  0.03655614  0.02235711 -0.01168096\n",
      " -0.04695265  0.04094616 -0.01395918 -0.01067196 -0.04353678  0.06061899\n",
      " -0.02666739 -0.01143066 -0.00058592  0.06937121  0.05838364  0.02784518\n",
      "  0.058709    0.04042409  0.02073148  0.02366819  0.07633445  0.05465862\n",
      "  0.04965012 -0.05002634  0.01707626 -0.00284997]\n"
     ]
    }
   ],
   "source": [
    "#Word2Vec لتمثيل الكلمات العربية كمتجهات رقمية\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare the corpus: list of lists of words\n",
    "# If you have cleaned texts stored in the DataFrame `df`\n",
    "# (after confirming the correct column name)\n",
    "corpus = df['paragraph'].apply(lambda x: x.split()).tolist()  #مثال الإدخال: [\"العلم نور\", \"الجهل ظلام\"] , المخرجات: [[\"العلم\", \"نور\"], [\"الجهل\", \"ظلام\"]]\n",
    "\n",
    "\n",
    "# Train the Word2Vec model\n",
    "                       #عدد الكلمات المحيطة التي ينظر إليها النموذج    #الحد الأدنى لظهور الكلمة (1 تعني كل الكلمات)  \n",
    "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4) #عدد أنوية المعالج المستخدمة (لتسريع التدريب)workers=4 \n",
    "\n",
    "# Example: Get the vector for a specific word\n",
    "try:\n",
    "    vector = model.wv['لغة']  # Replace 'لغة' with any Arabic word you want\n",
    "    print(vector)\n",
    "except KeyError:\n",
    "    print(\"The word 'لغة' was not found in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ba904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the selected model above, BERT, falls under:\n",
    "\n",
    "# \"deep learning architectures (e.g., Transformer)\"\n",
    "\n",
    "# ✅ Here's the match:\n",
    "# BERT stands for Bidirectional Encoder Representations from Transformers.\n",
    "\n",
    "# It is not a shallow model (like feedforward networks).\n",
    "\n",
    "# It is not based on LSTMs or RNNs.\n",
    "\n",
    "# It is a Transformer-based deep learning model, which is exactly what your requirement is referring to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c2320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b233f56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
