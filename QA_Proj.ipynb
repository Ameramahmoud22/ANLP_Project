{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7cee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<!-- ########################################################## -->\n",
      "<!-- #                                                          -->\n",
      "<!-- #  arwiki-20180920.xml                                     -->\n",
      "<!-- #                                                          -->\n",
      "<!-- #                                                          -->\n",
      "<!-- #  (C) Copyright 2018 Wikimedia Foundation, Inc.           -->\n",
      "<!-- #                        http://ar.wikipedia.org           -->\n",
      "<!-- #  (C) Copyright this adaption: Peter Kolb                 -->\n",
      "<!-- #               peter.kolb@linguatools.org                 -->\n",
      "<!-- #               http://www.linguatools.org/tools/corpora/  -->\n",
      "<!-- #                                                          -->\n",
      "<!-- #  This work is made available under the Creative Commons  -->\n",
      "<!-- #  Attribution-ShareAlike 3.0 License:                     -->\n",
      "<!-- #  http://creativecommons.org/licenses/by-sa/3.0/legalcode -->\n",
      "<!-- #   \n"
     ]
    }
   ],
   "source": [
    "file_path = \"arwiki-20180920-corpus.xml\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:1000])  # Print first 1000 characters to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b8ba86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "# Download NLTK Arabic resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize Arabic stopwords and stemmer\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "def preprocess_arabic(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove non-Arabic characters (keep only Arabic letters and spaces)\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize (split by spaces)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in arabic_stopwords]\n",
    "    \n",
    "    # Apply stemming\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5078a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_paragraphs = []\n",
    "\n",
    "file_path = 'arwiki-20180920-corpus.xml' \n",
    "\n",
    "MAX_LINES = 10000  # Limit lines for quick testing\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i >= MAX_LINES:\n",
    "            break\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip XML tags, HTML comments, and empty lines\n",
    "        if line.startswith('<?xml') or line.startswith('<!--') or line.startswith('<') or line == '':\n",
    "            continue\n",
    "\n",
    "        # Now clean and save real Arabic text\n",
    "        clean_line = preprocess_arabic(line)\n",
    "        \n",
    "        # Only add non-empty cleaned text\n",
    "        if clean_line.strip() != '':\n",
    "            clean_paragraphs.append(clean_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "243a8a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Finished preprocessing! Total cleaned paragraphs: 814\n"
     ]
    }
   ],
   "source": [
    "print(f\" Finished preprocessing! Total cleaned paragraphs: {len(clean_paragraphs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2637172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame({'paragraph': clean_paragraphs})\n",
    "df = df.fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7a7f56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned Arabic sample saved successfully to cleaned_arabic_wiki_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned sample\n",
    "df.to_csv('cleaned_arabic_wiki_sample.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\" Cleaned Arabic sample saved successfully to cleaned_arabic_wiki_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6144bf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample Context:\n",
      " زخر ياه ارض سطح بعد شكل تنع حيي يه، نقص زدد عمق عمد حية ائة بحر حيط وجد ماء كعمل سسي كتل حيوية، ويك عمل حدد ناج كمة قدر غذي نبت ذاب وسف ركب ترج مونيوم نتر ضفة ثني كسد كرب\n"
     ]
    }
   ],
   "source": [
    "# Print a sample paragraph\n",
    "if len(clean_paragraphs) > 0:\n",
    "    print(\"\\n Sample Context:\\n\", clean_paragraphs[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a8cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2288ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['paragraph'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715078a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(814, 4111)\n",
      "['آبر' 'آتم' 'آخر' ... 'يوم' 'يون' 'يوه']\n",
      "Cleaned Arabic sample saved successfully to cleaned_arabic_wiki_sample.csv\n",
      "\n",
      "Sample Context:\n",
      " زخر ياه ارض سطح بعد شكل تنع حيي يه، نقص زدد عمق عمد حية ائة بحر حيط وجد ماء كعمل سسي كتل حيوية، ويك عمل حدد ناج كمة قدر غذي نبت ذاب وسف ركب ترج مونيوم نتر ضفة ثني كسد كرب\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = df['paragraph'].tolist()  # Use all cleaned texts from DataFrame\n",
    "\n",
    "# Initialize TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus)\n",
    "\n",
    "print(X.shape)  # Number of documents x number of features\n",
    "print(tfidf.get_feature_names_out())  # See the vocabulary\n",
    "\n",
    "# Save cleaned sample\n",
    "df.to_csv('cleaned_arabic_wiki_sample.csv', index=False, encoding='utf-8')\n",
    "print(\"Cleaned Arabic sample saved successfully to cleaned_arabic_wiki_sample.csv\")\n",
    "\n",
    "# Print a sample paragraph\n",
    "if len(clean_paragraphs) > 6:  # Notice: check if index 6 exists\n",
    "    print(\"\\nSample Context:\\n\", clean_paragraphs[6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05310d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db32f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02465982  0.02210858  0.02295838  0.00122577  0.00963704 -0.07336017\n",
      "  0.04456649  0.10314479 -0.03340747 -0.02782421 -0.03188172 -0.05230464\n",
      " -0.02025422  0.01345363 -0.00024046 -0.04092807 -0.00242839 -0.04591601\n",
      "  0.00911411 -0.09710969  0.01579804  0.01291465  0.03608207 -0.01952653\n",
      " -0.00371001 -0.0189515  -0.04029905 -0.04442123 -0.03824596 -0.00056775\n",
      "  0.04286265  0.01902711  0.00808944 -0.03060136 -0.00857342  0.0588086\n",
      " -0.01053068 -0.03707424 -0.02983358 -0.08111031 -0.01265097 -0.04168354\n",
      " -0.0084947   0.01928122  0.04559947 -0.03364554 -0.03676334 -0.00363762\n",
      "  0.04084597  0.04047025  0.02170288 -0.0373952   0.00470576  0.01812375\n",
      " -0.04285415  0.04288952  0.00936949  0.00376055 -0.05506981  0.01030207\n",
      "  0.01562598  0.02727741 -0.01852809 -0.02495131 -0.07165431  0.02490312\n",
      "  0.02929877  0.04068363 -0.05740259  0.04528158 -0.0264707   0.02896178\n",
      "  0.04488745 -0.02868368  0.04104752  0.03655614  0.02235711 -0.01168096\n",
      " -0.04695265  0.04094616 -0.01395918 -0.01067196 -0.04353678  0.06061899\n",
      " -0.02666739 -0.01143066 -0.00058592  0.06937121  0.05838364  0.02784518\n",
      "  0.058709    0.04042409  0.02073148  0.02366819  0.07633445  0.05465862\n",
      "  0.04965012 -0.05002634  0.01707626 -0.00284997]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare the corpus: list of lists of words\n",
    "# If you have cleaned texts stored in the DataFrame `df`\n",
    "# (after confirming the correct column name)\n",
    "corpus = df['paragraph'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Example: Get the vector for a specific word\n",
    "try:\n",
    "    vector = model.wv['لغة']  # Replace 'لغة' with any Arabic word you want\n",
    "    print(vector)\n",
    "except KeyError:\n",
    "    print(\"The word 'لغة' was not found in the vocabulary.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
